# -*- coding: utf-8 -*-
"""Trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qZoH8ZWAJjZ5tTrjcigugK7kw1ez4I-M
"""

from copy import deepcopy
from tqdm import tqdm_notebook

import numpy as np

import torch
import torch.nn.functional as F
import torch.optim as optim

class Trainer():

  def __init__(self, model, optimizer, crit):
    self.model=model
    self.optimizer=optimizer
    self.crit=crit

    super().__init__()

  def _train(self, x, y, config):
    self.model.train()
    lookback=config.lookback

    #Shuffle before begin and make mini batches
    #config.batch_size=1(여기서는 sample_id 하나에 대해서 mini_batch로 놓을 것임)
    indices=torch.randperm(x.size(0), device=x.device)
    x=torch.index_select(x, dim=0, index=indices).split(config.batch_size, dim=0) #[1, 1380, 5]가 train_cnt만큼 생김
    y=torch.index_select(y, dim=0, index=indices).split(config.batch_size, dim=0) #[1, 120, 5]가 train_cnt만큼 생김

    total_loss=0

    for i, (x_i, y_i) in tqdm_bar(enumerate(zip(x, y))):
      #x_i : [1, 1380, 5], y_i : [1, 120, 5]
      x_series=pd.DataFrame(x_i)

      # 1380분을 LSTM 입력값으로 쪼개기
      x_train, y_train = split_data(x_series, lookback) #x_train, y_train은 numpy 형태

      x_train = torch.from_numpy(x_train).type(torch.Tensor) #(batch_size, time_seq,n_feature)=(1361,19,5)

      y_train=y_train[:,:].reshape([-1,n_feature])
      y_train_lstm1= torch.from_numpy(y_train).type(torch.Tensor) #(batch_size, time_seq,n_feature)=(1361,1)
      y_train_lstm2=y_i.view([-1,n_feature]) ##(batch_size, time_seq,n_feature)=(120,1)
      y_train_lstm=torch.cat([y_train_lstm1, y_train_lstm2], dim=0) ##(batch_size, time_seq,n_feature)=(1481,1)

      # Model에 넣기
      y_train_pred = self.model(x=x_train, data_raw=x_series, lookback=lookback)

      #loss = criterion(y_train_pred, y_train_lstm)
      y_train_pred=y_train_pred.view([1,-1]) # (1361,5)->(1,1361*5), y_train_pred.view([1,-1,n_feature])
      y_train_lstm=y_train_lstm.view([1,-1]) # (1361,5)->(1,1361*5), y_train_lstm.view([1,-1,n_feature])
      loss_i = self.crit(y_train_pred, y_train_lstm)

      self.optimizer.zero_grad()
      loss_i.backward()
      self.optimizer.step()

      if config.verbose>=2:
        print("Train Iteration(%d/%d): loss=%.4e" % (i + 1, len(x), float(loss_i)))
      
      # Don't forget to detach to prevent memory leak.
      total_loss += float(loss_i)

    return total_loss/len(x)

  def _validate(self, x, y, config):
    #Turn evaluation mode on.
    self.model.eval()

    # Turn on the no_grad mode to make more efficiently
    with torch.no_grad():
      #Shuffle before begin
      indices=torch.randperm(x.size(0), device=x.device)
      x=torch.index_select(x, dim=0, index=indices).split(config.batch_size, dim=0) #[1, 1380, 5]가 valid_cnt만큼 생김
      y=torch.index_select(y, dim=0, index=indices).split(config.batch_size, dim=0) #[1, 120, 5]가 valid_cnt만큼 생김

      total_loss=0

      for i, (x_i, y_i) in tqdm_notebook(enumerate(zip(x, y))):
        #x_i : [1, 1380, 5], y_i : [1, 120, 5]
        x_series=pd.DataFrame(x_i)

        # 1380분을 LSTM 입력값으로 쪼개기
        x_train, y_train = split_data(x_series, lookback) #x_train, y_train은 numpy 형태

        x_train = torch.from_numpy(x_train).type(torch.Tensor) #(batch_size, time_seq,n_feature)=(1361,19,5)

        y_train=y_train[:,:].reshape([-1,n_feature])
        y_train_lstm1= torch.from_numpy(y_train).type(torch.Tensor) #(batch_size, time_seq,n_feature)=(1361,1)
        y_train_lstm2=y_i.view([-1,n_feature]) ##(batch_size, time_seq,n_feature)=(120,1)
        y_train_lstm=torch.cat([y_train_lstm1, y_train_lstm2], dim=0) ##(batch_size, time_seq,n_feature)=(1481,1)

        # Model에 넣기
        y_train_pred = self.model(x=x_train, data_raw=x_series, lookback=lookback)

        #loss = criterion(y_train_pred, y_train_lstm)
        y_train_pred=y_train_pred.view([1,-1]) # (1361,5)->(1,1361*5), y_train_pred.view([1,-1,n_feature])
        y_train_lstm=y_train_lstm.view([1,-1]) # (1361,5)->(1,1361*5), y_train_lstm.view([1,-1,n_feature])
        loss_i = self.crit(y_train_pred, y_train_lstm)


        if config.verbose>=2:
          print("Train Iteration(%d/%d): loss=%.4e" % (i + 1, len(x), float(loss_i)))
        
        # Don't forget to detach to prevent memory leak.
        total_loss += float(loss_i)

    return total_loss/len(x) 

  def train(self, train_data, valid_data, config):
    lowest_loss=np.inf
    best_model=None

    for epoch_index in tqdm_notebook(range(config.n_epochs)):
      train_loss=self._train(train_data[0],train_data[1], config)
      valid_loss=self._validate(valid_data[0], valid_data[1], config)

      # You must use deep copy to take a snapshot of current best weights
      if valid_loss<=lowest_loss:
        lowest_loss = valid_loss
        best_model = deepcopy(self.model.state_dict())
      
      print("Epoch(%d/%d): train_loss=%.4e  valid_loss=%.4e  lowest_loss=%.4e" % (
          epoch_index +1,
          config.n_epochs,
          train_loss,
          valid_loss,
          lowest_loss))
    
    # Restore to best model.
    self.model.load_state_dict(best_model) #best model(state_dict)를 model에 저장하는 것