# -*- coding: utf-8 -*-
"""rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Eal9_IBmrI-Kszn0iXIb9d6k1qKKzMdQ
"""

import torch
import torch.nn as nn

## backward 실행 가능하도록 수정본
class LSTM(nn.Module):
  def __init__(
      self,
      input_dim,
      hidden_dim,
      num_layers,
      output_dim,
      dropout_p=False):
    
    self.input_dim=input_dim
    self.hidden_dim=hidden_dim
    self.num_layers=num_layers
    self.output_dim=output_dim
    self.dropout_p=dropout_p
    self.outputs=[]

    super().__init__()

    self.lstm=nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, 
                      num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_p)
    sself.layers=nn.Sequential(
        nn.ReLU(),
        nn.BatchNorm1d(hidden_size*2),
        nn.Linear(hidden_dim*2, output_dim)
    ) 
    
    
  
  def forward(self, x, data_raw, lookback): #x =x_train, data_raw=x_series, lookback=주기
    outputs=[]
    for i, input_t in enumerate(x.chunk(x.size(0), dim=0)): #i=(1,lookback-1,n_feature)
      # |x|=(batch_size, time_step, vec_size)
      x,_=self.lstm(input_t)

      #|x|=(batch_size, time_step, hidden_size*2)
      y= self.layers(x[:,-1])
   
      #|y|=(batch_size, output_dim=n_feature)
      outputs += [y] #outputs : [[1], [2], ...] 형태


    #예측값을 입력값으로 넣을 준비
    future=120
    data_raw = data_raw.to_numpy() # df convert to numpy array

    #예측값을 입력값으로 넣어주기
    for index in range(len(data_raw)-lookback+1,len(data_raw)+future-lookback+1): #range(1361, 1481)
      if index<len(data_raw):
        idx=(index-(len(data_raw)-lookback+1))
        # 1361~1379 넣고 1380 예측으로 해야함
        if idx==0:
          train_ft=torch.from_numpy(data_raw[index:len(data_raw)]).type(torch.Tensor)
          train_ft=train_ft.view([-1,lookback-1,input_dim])

          # |x|=(batch_size, time_step, vec_size)
          train_ft, _=self.lstm(train_ft)

          #|x|=(batch_size, time_step, hidden_size*2)
          y= self.layers(train_ft[:,-1])

          #|y|=(batch_size, value)
          outputs += [y]  #outputs : [[1], [2], ...] 형태


        # 기존의 값들과 예측값을 혼재한 경우
        else:
          a=torch.from_numpy(data_raw[index:len(data_raw)]).type(torch.Tensor) # (lookback-1, input_dim=feature_n)
          b=torch.cat(outputs[-idx:], dim=0).view([-1,input_dim]) # (lookback-1,  input_dim=feature_n)
          train_ft=torch.cat([a,b],dim=0).view([-1,lookback-1,input_dim])

          # |x|=(batch_size, time_step, vec_size)
          train_ft, _=self.lstm(train_ft)

          #|x|=(batch_size, time_step, hidden_size*2)
          y= self.layers(train_ft[:,-1])

          #|y|=(batch_size, value)
          outputs += [y]  #outputs : [[1], [2], ...] 형태

      # 예측값들로만 이루어진 경우
      else:
        idx=lookback-1
        train_ft=torch.cat(outputs[-idx:], dim=0).view([-1,lookback-1,input_dim])

        # |x|=(batch_size, time_step, vec_size)
        train_ft, _=self.lstm(train_ft)

        #|x|=(batch_size, time_step, hidden_size*2)
        y= self.layers(train_ft[:,-1])

        #|y|=(batch_size, value)
        outputs += [y]  #outputs : [[1], [2], ...] 형태

    outputs = torch.stack(outputs,1).squeeze(0) #list형태를 torch 형태로 바꿔주기, (1361,output_dim=n_feature)
    ##outputs = torch.as_tensor(outputs) list형태를 torch 형태로 바꿔주는 것을 다음 코드로 했다가 loss.backward, optimizer.step이 작동하지 않았음
    #{outputs]=(batch_size, value)


    return outputs